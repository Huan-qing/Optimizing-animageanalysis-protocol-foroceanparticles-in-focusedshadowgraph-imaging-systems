{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Deep Learning for Vision Systems](https://www.manning.com/books/deep-learning-for-vision-systems?a_aid=compvisionbookcom&a_bid=90abff15) Book\n",
    "\n",
    "\n",
    "## Chapter 6 Project: Sign language exercise\n",
    "\n",
    "---\n",
    "### 1. Import the libraries that we need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.applications import imagenet_utils\n",
    "from keras.applications import vgg16\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.metrics import categorical_crossentropy\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Flatten, Dropout, BatchNormalization\n",
    "from keras.models import Model\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path  = 'data/train'\n",
    "valid_path  = 'data/valid'\n",
    "test_path  = 'data/test'\n",
    "N=50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 416 images belonging to 2 classes.\n",
      "Found 60 images belonging to 2 classes.\n",
      "Found 118 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# ImageDataGenerator generates batches of tensor image data with real-time data augmentation. \n",
    "# The data will be looped over (in batches).\n",
    "# in this example, we won't be doing any image augmentation\n",
    "train_batches = ImageDataGenerator().flow_from_directory(train_path, \n",
    "                                                         target_size=(1280,960), \n",
    "                                                         batch_size=30\n",
    "                                                         )\n",
    "\n",
    "valid_batches = ImageDataGenerator().flow_from_directory(valid_path,\n",
    "                                                         target_size=(1280,960), \n",
    "                                                         batch_size=30)\n",
    "\n",
    "test_batches = ImageDataGenerator().flow_from_directory(test_path, \n",
    "                                                        target_size=(1280,960), \n",
    "                                                        batch_size=30, \n",
    "                                                        shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. VGG16 base model pre-trained on ImageNet dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1280, 960, 3)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 1280, 960, 64)     1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 1280, 960, 64)     36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 640, 480, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 640, 480, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 640, 480, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 320, 240, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 320, 240, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 160, 120, 256)     0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 160, 120, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 80, 60, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 40, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = vgg16.VGG16(weights = \"imagenet\", include_top=False, input_shape = (1280,960,3), pooling='avg')\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. freeze the classification layers in the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1280, 960, 3)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 1280, 960, 64)     1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 1280, 960, 64)     36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 640, 480, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 640, 480, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 640, 480, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 320, 240, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 320, 240, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 160, 120, 256)     0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 160, 120, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 80, 60, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 40, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 7,079,424\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# iterate through its layers and lock them to make them not trainable with this code\n",
    "for layer in base_model.layers[:-5]:\n",
    "    layer.trainable = False\n",
    "\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 1280, 960, 3)      0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 1280, 960, 64)     1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 1280, 960, 64)     36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 640, 480, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 640, 480, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 640, 480, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 320, 240, 128)     0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 320, 240, 256)     295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 320, 240, 256)     590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 160, 120, 256)     0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 160, 120, 512)     1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 160, 120, 512)     2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 80, 60, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 80, 60, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 40, 30, 512)       0         \n",
      "_________________________________________________________________\n",
      "global_average_pooling2d_1 ( (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "softmax (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 14,715,714\n",
      "Trainable params: 7,080,450\n",
      "Non-trainable params: 7,635,264\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# use “get_layer” method to save the last layer of the network\n",
    "last_layer = base_model.get_layer('global_average_pooling2d_1')\n",
    "\n",
    "# save the output of the last layer to be the input of the next layer\n",
    "last_output = last_layer.output\n",
    "\n",
    "# add our new softmax layer with 3 hidden units\n",
    "x = Dense(2, activation='softmax', name='softmax')(last_output)\n",
    "\n",
    "# instantiate a new_model using keras’s Model class\n",
    "new_model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# print the new_model summary\n",
    "new_model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Train the new model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_model.compile(Adam(lr=0.00001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "checkpointer = ModelCheckpoint(filepath='schlieren.model.hdf5', save_best_only=True)\n",
    "\n",
    "history = new_model.fit_generator(train_batches, steps_per_epoch=18,\n",
    "                   validation_data=valid_batches, validation_steps=3, epochs=N, verbose=1, callbacks=[checkpointer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Training Loss on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"upper right\")\n",
    "plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), history.history[\"accuracy\"], label=\"train_acc\")\n",
    "plt.plot(np.arange(0, N), history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "plt.title(\"Accuracy on Dataset\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig(\"accu.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path):\n",
    "    data = load_files(path)\n",
    "    paths = np.array(data['filenames'])\n",
    "    targets = np_utils.to_categorical(np.array(data['target']))\n",
    "    return paths, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "test_files, test_targets = load_dataset('data/test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 118/118 [00:02<00:00, 57.53it/s]\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "\n",
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(1280,960,3))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (224, 224, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 224, 224, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def paths_to_tensor(img_paths):\n",
    "    list_of_tensors = [path_to_tensor(img_path) for img_path in tqdm(img_paths)]\n",
    "    return np.vstack(list_of_tensors)\n",
    "\n",
    "test_tensors = preprocess_input(paths_to_tensor(test_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118/118 [==============================] - 14s 121ms/step\n",
      "\n",
      "Testing loss: 0.9458\n",
      "Testing accuracy: 0.7203\n"
     ]
    }
   ],
   "source": [
    "new_model = load_model('./1st/schlieren.model.hdf5')\n",
    "# new_model = load_model('/home/hhuan006/Desktop/schlieren project/rio1/result/result_721/my_best_model.hdf5')\n",
    "print('\\nTesting loss: {:.4f}\\nTesting accuracy: {:.4f}'.format(*new_model.evaluate(test_tensors, test_targets, batch_size=10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. create the confusion matrix to evaluate the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.preprocessing import image  \n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from tqdm import tqdm\n",
    "from sklearn.datasets import load_files\n",
    "from keras.utils import np_utils\n",
    "import numpy as np\n",
    "\n",
    "import cv2\n",
    "\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def path_to_tensor(img_path):\n",
    "    # loads RGB image as PIL.Image.Image type\n",
    "    img = image.load_img(img_path, target_size=(1280,960,3))\n",
    "    # convert PIL.Image.Image type to 3D tensor with shape (1280,960, 1280,960, 3)\n",
    "    x = image.img_to_array(img)\n",
    "    # convert 3D tensor to 4D tensor with shape (1, 1280,960, 1280,960, 3) and return 4D tensor\n",
    "    return np.expand_dims(x, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir = 'data/test'\n",
    "tp=0\n",
    "tn=0\n",
    "fp=0\n",
    "fn=0\n",
    "t_path = Path(test_dir)\n",
    "new_model = load_model('./1st/schlieren.model.hdf5')\n",
    "classlist = [str(p.absolute()) for p in list(t_path.glob('*'))]\n",
    "# print(os.path.basename(folderpath))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,clas in enumerate(classlist):\n",
    "    testlist = os.listdir(clas)\n",
    "#     print(testlist)\n",
    "#     print(i)\n",
    "    for ea in testlist:\n",
    "        \n",
    "        img_name = path_to_tensor(clas+'/'+ea)\n",
    "        name_str = os.path.basename(str(ea))\n",
    "    #     im = cv2.imread(img)\n",
    "    #     x = image.img_to_array(im)\n",
    "    #     x = np.expand_dims(im, axis=0)\n",
    "        result_pre = new_model.predict(img_name)\n",
    "    #     print(result_pre.size)\n",
    "#         print(result_pre)\n",
    "    #     if result_pre[0][1]>0.75:\n",
    "        if result_pre[0][1]>result_pre[0][0] and i == 0:\n",
    "            tp+=1\n",
    "        if result_pre[0][1]<result_pre[0][0] and i == 0:\n",
    "            fn+=1\n",
    "        if result_pre[0][1]>result_pre[0][0] and i == 1:\n",
    "            fp+=1\n",
    "        if result_pre[0][1]<result_pre[0][0] and i == 1:\n",
    "            tn+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[56, 3], [0, 59]]\n"
     ]
    }
   ],
   "source": [
    "cm = [[tp,   fn],\n",
    "       [  fp,  tn]]\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline \n",
    "# above is necessary to visualize the graph in jupyter notebook\n",
    "# Plot confusion matrix in a beautiful manner\n",
    "\n",
    "fig = plt.figure()\n",
    "ax= plt.subplot()\n",
    "sns.heatmap(cm, annot=True, ax = ax, fmt = 'g'); #annot=True to annotate cells\n",
    "# labels, title and ticks\n",
    "ax.set_xlabel('Predicted', fontsize=20)\n",
    "ax.xaxis.set_label_position('top') \n",
    "ax.xaxis.set_ticklabels(['schlieren', 'not'], fontsize = 15)\n",
    "# ax.xaxis.tick_top()\n",
    "\n",
    "ax.set_ylabel('True', fontsize=20)\n",
    "ax.yaxis.set_ticklabels(['schlieren', 'not'], fontsize = 15)\n",
    "\n",
    "plt.show()\n",
    "fig.savefig('matrix.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The End!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
